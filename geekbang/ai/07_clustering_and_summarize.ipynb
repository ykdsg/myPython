{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## 基于Embedding向量进行文本聚类"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "start_time": "2023-04-20T09:57:24.705508Z",
     "end_time": "2023-04-20T09:57:48.828390Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def twenty_newsgroup_to_csv():\n",
    "    newsgroups_train = fetch_20newsgroups(subset='train', remove=('headers', 'footers', 'quotes'))\n",
    "\n",
    "    df = pd.DataFrame([newsgroups_train.data, newsgroups_train.target.tolist()]).T\n",
    "    df.columns = ['text', 'target']\n",
    "\n",
    "    targets = pd.DataFrame(newsgroups_train.target_names, columns=['title'])\n",
    "\n",
    "    out = pd.merge(df, targets, left_on='target', right_index=True)\n",
    "    out.to_csv('20_newsgroup.csv', index=False)\n",
    "\n",
    "# 已经在本地生成了csv文件，不需要再次运行\n",
    "# twenty_newsgroup_to_csv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows before null filtering: 11314\n",
      "Number of rows before token number filtering: 11096\n",
      "Number of rows data used: 10640\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "from openai.embeddings_utils import get_embeddings\n",
    "import openai, os, tiktoken, backoff\n",
    "\n",
    "load_dotenv()\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "embedding_model = \"text-embedding-ada-002\"\n",
    "embedding_encoding = \"cl100k_base\"  # this the encoding for text-embedding-ada-002\n",
    "batch_size = 2000\n",
    "max_tokens = 1000  # the maximum for text-embedding-ada-002 is 8191\n",
    "\n",
    "df = pd.read_csv('20_newsgroup.csv')\n",
    "print(\"Number of rows before null filtering:\", len(df))\n",
    "df = df[df['text'].isnull() == False]\n",
    "encoding = tiktoken.get_encoding(embedding_encoding)\n",
    "\n",
    "df[\"n_tokens\"] = df.text.apply(lambda x: len(encoding.encode(x)))\n",
    "print(\"Number of rows before token number filtering:\", len(df))\n",
    "df = df[df.n_tokens <= max_tokens]\n",
    "print(\"Number of rows data used:\", len(df))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-20T10:21:07.432285Z",
     "end_time": "2023-04-20T10:21:16.104835Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 通过批量调用 Embedding 的接口，拿到文本的 Embedding 向量，然后把整个数据存储成 parquet 文件。\n",
    "对于这样的大数据集，不要存储成 CSV 格式。特别是我们获取到的 Embedding 数据，是很多浮点数，存储成 CSV 格式会把本来只需要 4 个字节的浮点数，都用字符串的形式存储下来，会浪费好几倍的空间，写入的速度也很慢。我在这里采用了 parquet 这个序列化的格式"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "# 以下代码比较消耗 Token，你可以不运行\n",
    "# @backoff.on_exception(backoff.expo, openai.error.RateLimitError)\n",
    "# def get_embeddings_with_backoff(prompts, engine):\n",
    "#     embeddings = []\n",
    "#     for i in range(0, len(prompts), batch_size):\n",
    "#         batch = prompts[i:i+batch_size]\n",
    "#         embeddings += get_embeddings(list_of_text=batch, engine=engine)\n",
    "#     return embeddings\n",
    "\n",
    "# prompts = df.text.tolist()\n",
    "# prompt_batches = [prompts[i:i+batch_size] for i in range(0, len(prompts), batch_size)]\n",
    "\n",
    "# embeddings = []\n",
    "# for batch in prompt_batches:\n",
    "#     batch_embeddings = get_embeddings_with_backoff(prompts=batch, engine=embedding_model)\n",
    "#     embeddings += batch_embeddings\n",
    "\n",
    "# df[\"embedding\"] = embeddings\n",
    "# df.to_parquet(\"data/20_newsgroup_with_embedding.parquet\", index=False)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-20T10:52:24.537555Z",
     "end_time": "2023-04-20T10:52:24.543540Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "通过 NumPy 的 stack 函数，把所有的 Embedding 放到一个矩阵里面，设置一下要聚合出来的类的数量，然后运行一下 K-Means 算法的 fit 函数，就可以得到每个文本所属的类别了。"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "embedding_df = pd.read_parquet(\"data/20_newsgroup_with_embedding.parquet\")\n",
    "\n",
    "matrix = np.vstack(embedding_df.embedding.values)\n",
    "num_of_clusters = 20\n",
    "\n",
    "kmeans = KMeans(n_clusters=num_of_clusters, init=\"k-means++\", n_init=10, random_state=42)\n",
    "kmeans.fit(matrix)\n",
    "labels = kmeans.labels_\n",
    "embedding_df[\"cluster\"] = labels"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-20T11:03:26.135263Z",
     "end_time": "2023-04-20T11:03:56.866420Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "data": {
      "text/plain": "                                                text  target      title  \\\n0  I was wondering if anyone out there could enli...       7  rec.autos   \n1  \\nIt depends on your priorities.  A lot of peo...       7  rec.autos   \n2  an excellent automatic can be found in the sub...       7  rec.autos   \n3  : Ford and his automobile.  I need information...       7  rec.autos   \n4  \\nYo! Watch the attributions--I didn't say tha...       7  rec.autos   \n\n   n_tokens                                          embedding  cluster  \n0       121  [-0.0391300804913044, 0.013502633199095726, -0...        5  \n1       108  [-0.0011249205563217402, -0.00376517535187304,...        5  \n2       476  [-0.018259447067975998, -0.008410007692873478,...        5  \n3        86  [-0.012589422054588795, 0.006539034191519022, ...        5  \n4       130  [-0.0006192282889969647, -0.011226896196603775...       10  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>text</th>\n      <th>target</th>\n      <th>title</th>\n      <th>n_tokens</th>\n      <th>embedding</th>\n      <th>cluster</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>I was wondering if anyone out there could enli...</td>\n      <td>7</td>\n      <td>rec.autos</td>\n      <td>121</td>\n      <td>[-0.0391300804913044, 0.013502633199095726, -0...</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>\\nIt depends on your priorities.  A lot of peo...</td>\n      <td>7</td>\n      <td>rec.autos</td>\n      <td>108</td>\n      <td>[-0.0011249205563217402, -0.00376517535187304,...</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>an excellent automatic can be found in the sub...</td>\n      <td>7</td>\n      <td>rec.autos</td>\n      <td>476</td>\n      <td>[-0.018259447067975998, -0.008410007692873478,...</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>: Ford and his automobile.  I need information...</td>\n      <td>7</td>\n      <td>rec.autos</td>\n      <td>86</td>\n      <td>[-0.012589422054588795, 0.006539034191519022, ...</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>\\nYo! Watch the attributions--I didn't say tha...</td>\n      <td>7</td>\n      <td>rec.autos</td>\n      <td>130</td>\n      <td>[-0.0006192282889969647, -0.011226896196603775...</td>\n      <td>10</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_df.head()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-20T11:04:02.230937Z",
     "end_time": "2023-04-20T11:04:02.236207Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 统计每个cluster的数量\n",
    "new_df = embedding_df.groupby('cluster')['cluster'].count().reset_index(name='count')\n",
    "\n",
    "# 统计这个cluster里最多的分类的数量\n",
    "title_count = embedding_df.groupby(['cluster', 'title']).size().reset_index(name='title_count')\n",
    "first_titles = title_count.groupby('cluster').apply(lambda x: x.nlargest(1, columns=['title_count']))\n",
    "first_titles = first_titles.reset_index(drop=True)\n",
    "new_df = pd.merge(new_df, first_titles[['cluster', 'title', 'title_count']], on='cluster', how='left')\n",
    "new_df = new_df.rename(columns={'title': 'rank1', 'title_count': 'rank1_count'})\n",
    "\n",
    "# 统计这个cluster里第二多的分类的数量\n",
    "second_titles = title_count[~title_count['title'].isin(first_titles['title'])]\n",
    "second_titles = second_titles.groupby('cluster').apply(lambda x: x.nlargest(1, columns=['title_count']))\n",
    "second_titles = second_titles.reset_index(drop=True)\n",
    "new_df = pd.merge(new_df, second_titles[['cluster', 'title', 'title_count']], on='cluster', how='left')\n",
    "new_df = new_df.rename(columns={'title': 'rank2', 'title_count': 'rank2_count'})\n",
    "new_df.fillna(0, inplace=True)\n",
    "new_df['per_1'] = (new_df['rank1_count'] / new_df['count']).map(lambda x: '{:.2%}'.format(x))\n",
    "new_df['per_1_2'] = ((new_df['rank1_count'] + new_df['rank2_count']) / new_df['count']).map(\n",
    "    lambda x: '{:.2%}'.format(x))\n",
    "\n",
    "# 将缺失值替换为 0\n",
    "# 输出结果\n",
    "display(new_df)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 使用提示语对文本进行总结"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "items_per_cluster = 10\n",
    "COMPLETIONS_MODEL = \"text-davinci-003\"\n",
    "\n",
    "for i in range(num_of_clusters):\n",
    "    cluster_name = new_df[new_df.cluster == i].iloc[0].rank1\n",
    "    print(f\"Cluster {i}, Rank 1: {cluster_name}, Theme:\", end=\" \")\n",
    "\n",
    "    content = \"\\n\".join(\n",
    "        embedding_df[embedding_df.cluster == i].text.sample(items_per_cluster, random_state=42).values\n",
    "    )\n",
    "    response = openai.Completion.create(\n",
    "        model=COMPLETIONS_MODEL,\n",
    "        prompt=f'''我们想要给下面的内容，分组成有意义的类别，以便我们可以对其进行总结。请根据下面这些内容的共同点，总结一个50个字以内的新闻组的名称么？比如 “PC硬件”\\n\\n内容:\\n\"\"\"\\n{content}\\n\"\"\"新闻组名称：''',\n",
    "        temperature=0,\n",
    "        max_tokens=100,\n",
    "        top_p=1,\n",
    "    )\n",
    "    print(response[\"choices\"][0][\"text\"].replace(\"\\n\", \"\"))"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
