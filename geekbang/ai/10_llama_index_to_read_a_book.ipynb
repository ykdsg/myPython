{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "start_time": "2023-04-20T16:13:34.910629Z",
     "end_time": "2023-04-20T16:13:45.826627Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:llama_index.token_counter.token_counter:> [build_index_from_nodes] Total LLM token usage: 0 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [build_index_from_nodes] Total embedding token usage: 6763 tokens\n"
     ]
    }
   ],
   "source": [
    "import openai, load_api_key\n",
    "from llama_index import GPTSimpleVectorIndex, SimpleDirectoryReader, ServiceContext\n",
    "\n",
    "# 将整个./data/mr_fujino 的目录给加载进来。这里面的每一个文件，都会被当成是一篇文档。\n",
    "documents = SimpleDirectoryReader('./data/mr_fujino').load_data()\n",
    "# 将所有的文档交给了 GPTSimpleVectorIndex 构建索引，它会把文档分段转换成一个个向量，然后存储成一个索引。\n",
    "index = GPTSimpleVectorIndex.from_documents(documents)\n",
    "\n",
    "index.save_to_disk('index_mr_fujino.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:llama_index.token_counter.token_counter:> [query] Total LLM token usage: 2984 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [query] Total embedding token usage: 34 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "鲁迅先生在日本学习医学的老师是藤野严九郎先生。\n"
     ]
    }
   ],
   "source": [
    "# 把刚才生成的索引加载到内存里面来。\n",
    "index = GPTSimpleVectorIndex.load_from_disk('index_mr_fujino.json')\n",
    "#  实际会调用openAI，但会通过 Embedding 相似度找出来的内容填入到问题中。\n",
    "response = index.query(\"鲁迅先生在日本学习医学的老师是谁？\")\n",
    "print(response)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-20T16:21:56.561953Z",
     "end_time": "2023-04-20T16:22:01.492351Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:llama_index.token_counter.token_counter:> [query] Total LLM token usage: 2968 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [query] Total embedding token usage: 25 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "鲁迅先生去仙台的医学专门学校学习医学。\n"
     ]
    }
   ],
   "source": [
    "response = index.query(\"鲁迅先生去哪里学的医学？\", verbose=True)\n",
    "print(response)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-20T16:32:04.828603Z",
     "end_time": "2023-04-20T16:32:09.543005Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 实际通过上下文让AI 来回答\n",
    "1. 这个模版的开头，我们告诉 AI，我们为 AI 提供了一些上下文信息（Context information）。\n",
    "2. 模版里面支持两个变量，一个叫做 context_str，另一个叫做 query_str。context_str 的地方，在实际调用的时候，会被通过 Embedding 相似度找出来的内容填入。而 query_str 则是会被我们实际提的问题替换掉。\n",
    "3. 实际提问的时候，我们告诉 AI，只考虑上下文信息，而不要根据自己已经有的先验知识（prior knowledge）来回答问题。"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:llama_index.token_counter.token_counter:> [query] Total LLM token usage: 2968 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [query] Total embedding token usage: 25 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "鲁迅先生去仙台的医学专门学校学习医学。\n"
     ]
    }
   ],
   "source": [
    "from llama_index import QuestionAnswerPrompt\n",
    "\n",
    "query_str = \"鲁迅先生去哪里学的医学？\"\n",
    "DEFAULT_TEXT_QA_PROMPT_TMPL = (\n",
    "    \"Context information is below. \\n\"\n",
    "    \"---------------------\\n\"\n",
    "    \"{context_str}\"\n",
    "    \"\\n---------------------\\n\"\n",
    "    \"Given the context information and not prior knowledge, \"\n",
    "    \"answer the question: {query_str}\\n\"\n",
    ")\n",
    "QA_PROMPT = QuestionAnswerPrompt(DEFAULT_TEXT_QA_PROMPT_TMPL)\n",
    "\n",
    "response = index.query(query_str, text_qa_template=QA_PROMPT, verbose=True)\n",
    "print(response)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-20T16:34:36.705764Z",
     "end_time": "2023-04-20T16:34:40.317771Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 通过llama_index对于文章进行小结"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:llama_index.llm_predictor.base:Unknown max input size for gpt-3.5-turbo, using defaults.\n",
      "INFO:llama_index.token_counter.token_counter:> [build_index_from_nodes] Total LLM token usage: 0 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [build_index_from_nodes] Total embedding token usage: 0 tokens\n",
      "INFO:llama_index.indices.common_tree.base:> Building index from nodes: 2 chunks\n",
      "INFO:llama_index.token_counter.token_counter:> [query] Total LLM token usage: 10031 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [query] Total embedding token usage: 0 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "鲁迅先生在日本仙台的医学专门学校学习时，遇到了藤野严九郎教授。藤野教授很有学问，对学生也很关心，甚至帮助鲁迅修改讲义。但鲁迅当时不够用功，有时也很任性。最终，他决定离开医学，去学习生物学。在他离开仙台之前，藤野先生给了他一张照片，并希望他能够保持联系。鲁迅先生很久没有和一个人通信了，尽管他想写信，但是他不知道该写些什么。他想起了一个人，这个人是他的老师，他对鲁迅很热心，给他很多鼓励和教诲。这个老师希望中国有新的医学，他的性格很伟大，但是他的名字并不为人所知。鲁迅曾经收藏了他的讲义，但是在搬家的时候丢失了。他的照片还挂在鲁迅的房间里，每当鲁迅疲倦的时候，看到他的照片就会感到勇气和良心发现。\n"
     ]
    }
   ],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.text_splitter import SpacyTextSplitter\n",
    "from llama_index import GPTListIndex, LLMPredictor, ServiceContext\n",
    "from llama_index.node_parser import SimpleNodeParser\n",
    "\n",
    "# define LLM\n",
    "# 设置模型输出的内容都在 1024 个 Token 以内，这样可以确保我们的小结不会太长，不会把一大段不相关的内容都合并到一起去。\n",
    "llm_predictor = LLMPredictor(llm=ChatOpenAI(temperature=0, model_name=\"gpt-3.5-turbo\", max_tokens=1024))\n",
    "\n",
    "# 进行中文文本的分割，这里用了中文的语言模型zh_core_web_sm\n",
    "# 限制了分割出来的文本段，最长不要超过 2048 个 Token\n",
    "text_splitter = SpacyTextSplitter(pipeline=\"zh_core_web_sm\", chunk_size=2048)\n",
    "parser = SimpleNodeParser(text_splitter=text_splitter)\n",
    "documents = SimpleDirectoryReader('./data/mr_fujino').load_data()\n",
    "nodes = parser.get_nodes_from_documents(documents)\n",
    "\n",
    "service_context = ServiceContext.from_defaults(llm_predictor=llm_predictor)\n",
    "# 建立索引的时候，并不会创建 Embedding，所以索引创建的时候很快，也不消耗 Token 数量\n",
    "list_index = GPTListIndex(nodes=nodes, service_context=service_context)\n",
    "\n",
    "response = list_index.query(\"下面鲁迅先生以第一人称‘我’写的内容，请你用中文总结一下:\", response_mode=\"tree_summarize\")\n",
    "print(response)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-20T16:56:18.885316Z",
     "end_time": "2023-04-20T16:57:29.334427Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
